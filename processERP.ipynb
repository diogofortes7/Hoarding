{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["import what we need to import"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import bioread\n", "import neurodsp\n", "import scipy.stats\n", "# Import spectral power functions\n", "from neurodsp.spectral import compute_spectrum, rotate_powerlaw"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Import utilities for loading and plotting data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from neurodsp.utils import create_times\n", "from neurodsp.utils.download import load_ndsp_data\n", "from neurodsp.plts.spectral import plot_power_spectra\n", "from neurodsp.plts.time_series import plot_time_series\n", "import bioread\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import mne\n", "import neurodsp\n", "# Import spectral power functions\n", "from neurodsp.spectral import compute_spectrum, rotate_powerlaw"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Import utilities for loading and plotting data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from neurodsp.utils import create_times\n", "from neurodsp.utils.download import load_ndsp_data\n", "from neurodsp.plts.spectral import plot_power_spectra\n", "from neurodsp.plts.time_series import plot_time_series\n", "# This file is included in bioread\n", "from mne.time_frequency import tfr_morlet, psd_multitaper, psd_welch\n", "# clean this up later\n", "import json\n", "import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## define some functions<br>\n", "unction for reading in the file and making it an MNE structure"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_fileACQ(aFileNameandPath):\n", "\tdataFile= bioread.read_file(aFileNameandPath)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tdataFile.channels\n", "\tdataFile.channels[2].name\n", "\tnp.shape(dataFile.channels[2].data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tsfreq = 2000  # Sampling frequency\n", "\ttimes = dataFile.channels[2].time_index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tchNames= [dataFile.channels[2].name,\n", "\t\t\t  dataFile.channels[3].name,\n", "\t\t\t  dataFile.channels[4].name,\n", "\t\t\t  dataFile.channels[7].name,\n", "\t\t\t  dataFile.channels[8].name,\n", "\t\t\t  dataFile.channels[9].name]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tchNames= ['Fz','O1', 'F3', 'O2', 'F4', 'Pz']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\teegMat =  np.array([dataFile.channels[2].data,\n", "\t\t\t\t\t\tdataFile.channels[3].data,\n", "\t\t\t\t\t\tdataFile.channels[4].data,\n", "\t\t\t\t\t\tdataFile.channels[7].data,\n", "\t\t\t\t\t\tdataFile.channels[8].data,\n", "\t\t\t\t\t\tdataFile.channels[9].data])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t#print(np.shape(eegMat))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# start MNE stuff\n", "\tchTypes = ['eeg' for x in chNames]# python is ugly beautiful.\n", "\tinfo = mne.create_info(ch_names=chNames, sfreq=sfreq, ch_types=chTypes)\n", "\tinfo.set_montage('standard_1020')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\traw = mne.io.RawArray(eegMat, info)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t########################\n", "\t#### GETTING EVENTS ####\n", "\t########################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tmyEvents = np.zeros((8, eegMat.shape[1]),dtype=int) #np.array([data.channels[ch].data for ch in range(13,21)])\n", "\tfor evChan in range(np.shape(myEvents)[0]):\n", "\t\tchanEvs = np.array(np.nonzero(dataFile.channels[evChan+13].data)[0])\n", "\t\t#print(chanEvs)\n", "\t\t#print(np.shape(chanEvs))\n", "\t\t#print('')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tmyEvents[evChan, chanEvs//1] = 1 << evChan # floor division gets us ints"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# Combine all event tracks into a single byte by summing\n", "\tbyteEvents = np.sum(myEvents, axis=0)\n", "\t#print(np.shape(byteEvents))\n", "\t#print(np.transpose(np.unique(byteEvents, return_counts=True)))\n", "\t#info.set_montage('standard_1020')\n", "\t#get missed events from log file"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tmissEvs = np.empty((3,),dtype = float)\n", "\tlogFile =aFileNameandPath[0:-3]\n", "\tlogFile= logFile+'log'\n", "\tprint(logFile)\n", "\tsrchForPhrase = \"The following output port codes were not sent because of a conflict on the port.\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\ttry:\n", "\t\twith open(logFile) as f:\n", "\t\t\tf = f.readlines()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tfoundSection = 0\n", "\t\tfor line in f:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tif foundSection == 1:\n", "\t\t\t\t#only works on lines with three numbers\n", "\t\t\t\ttry:\n", "\t\t\t\t\t#split and convert to float\n", "\t\t\t\t\tline_arr = np.array(line.split())\n", "\t\t\t\t\tline_val_arr = line_arr.astype(np.float)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\t\t\tif np.size(line_val_arr) == 3:\n", "\t\t\t\t\t\t#print(line_val_arr)\n", "\t\t\t\t\t\tmissEvs = np.append(missEvs, line_val_arr)\n", "\t\t\t\texcept:\n", "\t\t\t\t\t#print(\"not a line with numbers that we seek\")\n", "\t\t\t\t\ta=1+1 #python parsing forces me to put something here, can't have an empty except"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tif srchForPhrase in line:\n", "\t\t\t\t#start reading in lines next iteration\n", "\t\t\t\tfoundSection = 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tmissEvs = np.reshape(missEvs, (-1, 3))\n", "\t\tmissEvs = missEvs[1:][:]\n", "\t\t#print(missEvs)\n", "\t\t#combine the missed events with byteEvents\n", "\t\tbyteEvents_wMissed = byteEvents"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tfor missEv in missEvs:\n", "\t\t\tcode = int(missEv[1])\n", "\t\t\tlatency = int(missEv[2])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t\tbyteEvents_wMissed[latency] = code"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t#plt.plot(byteEvents_wMissed)\n", "\t\tprint(np.transpose(np.unique(byteEvents_wMissed, return_counts=True)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\tt_len = len(byteEvents_wMissed)/sfreq\n", "\t\tprint(\"initial (continuous) eeg epoch is \"+str(t_len)+\" seconds long!\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\texcept:\n", "\t\tprint(\"no log file found looking for \"+logFile)\n", "\t\tbyteEvents_wMissed = byteEvents"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# filtering here.\n", "\traw_filter = raw.copy().filter(l_freq=1, h_freq=20)\n", "\t# let's hack a sloppy edge detector here. no debounce...\n", "\t# loop through, if new is different than old keep new, if new is same as old set new to 0\n", "\toldInd = 0\n", "\tcleanEvents = byteEvents_wMissed.copy()\n", "\tfor i in range(len(cleanEvents)):\n", "\t\tnewEv  =cleanEvents[i].copy()\n", "\t\tif newEv == oldInd:\n", "\t\t\tcleanEvents[i] = 0 # if we leave this alone we'll only get the first occurance\n", "\t\tif newEv != oldInd:\n", "\t\t\toldInd = newEv\t\n", "\t\t# the best solution is usually the easiest\n", "\teventsArray = (np.array([range(len(raw.times)), cleanEvents]))\n", "\tsoftArray = eventsArray[:,cleanEvents != 0]\n", "\tsoftArray.shape[1]\n", "\tevb = \"blah\"\n", "\teventCode = 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t#4 bins\n", "\t# Paper go correct 24->95\n", "\t# Paper go incorrect 24 ->3\n", "\t# neutral correct 22 -> 95\n", "\t# neutral incorrect 22-> 3"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t#soft_arr_name = logFile+'_softarr.csv'\n", "\t#np.savetxt(soft_arr_name, softArray.T, delimiter=\",\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\teventsArray = []\n", "\tfor aRowIndex in range(softArray.shape[1]):\n", "\t\ttargTime = -1\n", "\t\tif softArray[1,aRowIndex] == 22 or softArray[1,aRowIndex] == 24:\n", "\t\t\ttargTime = softArray[0,aRowIndex]\n", "\t\t\t#print('target')\n", "\t\tif softArray[1,aRowIndex] == 187:\n", "\t\t\t#resting\n", "\t\t\ttheREsponseCode = softArray[1,aRowIndex]\n", "\t\t\tthisEventTime = softArray[0,aRowIndex]\n", "\t\t\teventCode = 5\n", "\t\t\teventsArray.append([thisEventTime,0,eventCode])\n", "\t\t\t#break # this is OK because we only enter the 2nd loop for responses\n", "\t\tif softArray[1,aRowIndex] == 3 or softArray[1,aRowIndex] == 95:\n", "\t\t\t#print(softArray[:,aRowIndex])\n", "\t\t\tthisResponse = softArray[:,aRowIndex]\n", "\t\t\ttheREsponseCode = softArray[1,aRowIndex]\n", "\t\t\tthisEventTime = softArray[0,aRowIndex]\n", "\t\t\t# now find the 22 or 24 before it\n", "\t\t\tfor anotherRowIndex in range(softArray.shape[1]):\n", "\t\t\t\tif softArray[0,anotherRowIndex] > softArray[0,aRowIndex]:\n", "\t\t\t\t\tbreak\n", "\t\t\t\tif softArray[1,anotherRowIndex] == 22 or softArray[1,anotherRowIndex] == 24:\n", "\t\t\t\t\teventBefore = softArray[1,anotherRowIndex]# we'll make sure that we're not going to\n", "\t\t\t\t\tevb = softArray[:,anotherRowIndex]\n", "\t\t\t\t\tresponseTime = thisEventTime\n", "\t\t\t\t\tif theREsponseCode == 95:\n", "\t\t\t\t\t\tif eventBefore == 22:\n", "\t\t\t\t\t\t\teventCode = 1\n", "\t\t\t\t\t\t\t#targTime = softArray[0,anotherRowIndex]\n", "\t\t\t\t\t\t\t#eventsArray.append([thisEventTime, 0,eventCode])\n", "\t\t\t\t\t\tif eventBefore == 24:\n", "\t\t\t\t\t\t\teventCode = 2\n", "\t\t\t\t\t\t\t#targTime = softArray[0,anotherRowIndex]\n", "\t\t\t\t\t\t\t#eventsArray.append([thisEventTime,0,eventCode])\n", "\t\t\t\t\tif theREsponseCode == 3:\n", "\t\t\t\t\t\tif eventBefore == 22:\n", "\t\t\t\t\t\t\teventCode = 3\n", "\t\t\t\t\t\t\t#targTime = softArray[0,anotherRowIndex]\n", "\t\t\t\t\t\t\t#eventsArray.append([thisEventTime,0,eventCode])\n", "\t\t\t\t\t\tif eventBefore == 24:\n", "\t\t\t\t\t\t\teventCode = 4\n", "\t\t\t\t\t\t\t#targTime = softArray[0,anotherRowIndex]\n", "\t\t\t\t\t\t\t#eventsArray.append([thisEventTime,0,eventCode])\n", "\t\t\teventsArray.append([thisEventTime,0,eventCode])\n", "\t\tif targTime > 0:\n", "\t\t\teventsArray.append([targTime,0,6])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\t# print(evb) # remember these fuckers are in half millisecond samples\n", "\t\t# print(eventCode + \",\" + str(thisEventTime))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\tearrray = np.array(eventsArray)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t#events_arr_name = logFile+'_evarr.csv'\n", "\t#np.savetxt(events_arr_name, earrray, delimiter=\",\")\n", "\t"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rint(earrray)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t\n", "\tevent_dict = {'neutralCorrect': 1, 'paperCorrect': 2, 'neutralError': 3,\n", "\t              'paperError': 4}\n", "\tpicks = mne.pick_types(info, meg=False, eeg=True, misc=False)\n", "\tpochs = mne.Epochs(raw_filter, earrray, tmin=-0.5 , tmax=0.35,event_id=event_dict, preload=True, baseline=(-0.5,-0.1))\n", "\tvent_dict = {'resting': 5}\n", "\trestingPochs = mne.Epochs(raw_filter, earrray, tmin=-0.0 , tmax=10,event_id=vent_dict, preload=True, baseline=(0,0))\n", "\t#raw.set_montage('standard_1020')\n", "\treject_criteria = dict(eeg= 125)  # 200 \u00b5V\n", "\t_ = pochs.drop_bad(reject=reject_criteria)\n", "\t_ = restingPochs.drop_bad(reject=reject_criteria)\n", "\tlookingFor = [5]  # start with resting rembember we recoaded these earlier\n", "\tprint(eventsArray)\n", "\t\n", "\ttimesforsegs =[]\n", "\ttagsforsegs =[]\n", "\tendtimes = []\n", "\tsegType='resting'\n", "\tfor i in range(0,len(eventsArray)):\n", "\t\tif eventsArray[i][2] in lookingFor:\n", "\t\t\tprint(eventsArray[i])\n", "\t\t\tprint(eventsArray[i][0]/2000)\n", "\t\t\ttimesforsegs.append(eventsArray[i][0]/2000)\n", "\t\t\ttagsforsegs.append(segType)\n", "\t\t\tif segType=='resting':\n", "\t\t\t\tif len(endtimes) >= 1:\n", "\t\t\t\t\tendtimes.append(eventsArray[i-1][0]/2000)\n", "\t\t\t\t\tprint(eventsArray[i-1])\n", "\t\t\t\t\tprint(eventsArray[i-1][0]/2000)\n", "\t\t\t\tendtimes.append(eventsArray[i][0]/2000 + 10)\n", "\t\t\tif 5 in lookingFor:\n", "\t\t\t\tlookingFor= [1,2,3,4]\n", "\t\t\t\tsegType='task'\n", "\t\t\telse:\n", "\t\t\t\tlookingFor = [5]\n", "\t\t\t\tsegType='resting'\n", "\tsegDF= pd.DataFrame(\n", "\t\t{'segType' : tagsforsegs,\n", "\t\t'segTime' : timesforsegs,\n", "\t\t'endTime' : endtimes}\n", "\t)\n", "\tprint(segDF)\n", "\t\t\n", "\t#for index, row in eventsArray.iterrows():\n", "\t#\tprint(row['c1'], row['c2'])\n", "\t\n", "\t\n", "\t\n", "\treturn pochs,restingPochs, segDF"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ead_fileACQ(testFile)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def pochs2json(someEpochs):\n", "\tchname = ['Fz','O1', 'F3', 'O2', 'F4', 'Pz']\n", "\t# this is ugly but we need those locations\n", "\t\n", "\tsds = mne.channels.make_standard_montage(kind = \"standard_1020\")\n", "\tsdsPOS = sds.get_positions()\n", "\tchanLocs = [sdsPOS['ch_pos'][x].tolist() for x in chname]\n", "\tchanLocs = []\n", "\tnewTimes =someEpochs.times.tolist()\n", "\tnewTimes2 =[x*1000 for x in newTimes]\n", "\tnewDict ={\n", "\t\t\"chans\" : someEpochs.ch_names,\n", "\t\t\"chanlocs\" :chanLocs,\n", "\t\t\"times\" : newTimes2,\n", "\t\t\"sampleRate\" : 2000, # prob should pull from structure, but so it goes\n", "\t\t\"bins\": [\n", "\t\t\t{\"name\":\"neutralCorrect\",\n", "\t\t\t \"bad\":1,\n", "\t\t\t \"good\":someEpochs['neutralCorrect'].get_data().shape[0],\n", "\t\t\t \"data\": someEpochs['neutralCorrect'].average().data.tolist()\n", "\t\t\t },\n", "\t\t\t{\"name\":\"neutralError\",\n", "\t\t\t \"bad\":1,\n", "\t\t\t \"good\":someEpochs['neutralError'].get_data().shape[0],\n", "\t\t\t \"data\":someEpochs['neutralError'].average().data.tolist()\n", "\t\t\t },\n", "\t\t\t{\"name\":\"paperCorrect\",\n", "\t\t\t \"bad\":1,\n", "\t\t\t \"good\":someEpochs['paperCorrect'].get_data().shape[0],\n", "\t\t\t \"data\":someEpochs['paperCorrect'].average().data.tolist()\n", "\t\t\t },\n", "\t\t\t{\"name\":\"paperError\",\n", "\t\t\t \"bad\":1,\n", "\t\t\t \"good\":someEpochs['paperError'].get_data().shape[0],\n", "\t\t\t \"data\":someEpochs['paperError'].average().data.tolist()\n", "\t\t\t }]\n", "\t}\n", "\treturn(newDict)"]}, {"cell_type": "markdown", "metadata": {}, "source": [""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataDir = \"/Users/diogo/Desktop/HoardingProc/new_subj\"\n", "import glob, os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["arr = os.listdir(dataDir)\n", "os.chdir(dataDir)\n", "for file in glob.glob(\"*.acq\"):\n", "\t\n", "\ttry:\n", "\t\tprint(file)\n", "\t\taaa, bbb, segTimes = read_fileACQ(dataDir +\"/\"+file)\n", "\t\tsmpledct = pochs2json(aaa)\n", "\t\tjsonName = file+\".json\"\n", "\t\tpsdName = file+\"_PSD.csv\"\n", "\t\talphaName =  file+\"_PSDalpha.csv\"\n", "\t\tsegTimeName  =  file+\"_segtimes.csv\"\n", "\t\twith open(jsonName, 'w') as outfile:\n", "\t\t\tjson.dump(smpledct, outfile)\n", "\t\t# now we need to dump the PSDs\n", "\t\tpsds, freqs = psd_multitaper(bbb, fmin=2, fmax=40, n_jobs=1)\n", "\t\tpsds = np.log10(psds)\n", "\t\tpsds_mean = np.transpose(psds.mean(0)) # n channels nephochs nfreqs\n", "\t\tdf = pd.DataFrame(psds_mean, columns = ['Fz','O1', 'F3', 'O2', 'F4', 'Pz'])\n", "\t\tdf['freq']= freqs\n", "\t\tdf.to_csv(psdName)\n", "\t\talphaDF = df[(df['freq'] > 7.8) & (df['freq'] < 12.5)]\n", "\t\t#print(df)\n", "\t\t#average em\n", "\t\talphaMean = alphaDF.mean(axis = 0) # got our alpha for each electrode now\n", "\t\talphaMean.to_csv(alphaName)\n", "\t\tsegTimes.to_csv(segTimeName)\n", "\t\t\n", "\texcept Exception as e:\n", "\t\tprint(e)\n", "\t\tprint(\"something failed\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}